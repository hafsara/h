Top, ta proposition va dans le bon sens. Voici ce que je te recommande pour qu’elle soit **propre, idempotente et 100% actif-actif**, avec des corrections concrètes sur ton code actuel.

---

# 0) Les 4 points clés (résumé)

1. **Workers Celery en parallèle** : OK, mets `worker_prefetch_multiplier=1` + `task_acks_late=True` pour éviter les doublons après crash.
2. **Beat unique ou distribué** : un seul Beat, ou RedBeat si tu veux la tolérance de panne du scheduler.
3. **DB** : continue à utiliser `FOR UPDATE SKIP LOCKED` et marque *immédiatement* le step “processing” sous transaction.
4. **App context Flask** : garde une Task Celery qui pousse le contexte Flask (sinon mails/DB plantent parfois).

---

# 1) `workflow/celery_app.py` — version actif-actif (corrigée + complète)

```python
# workflow/celery_app.py
from datetime import timedelta
import logging
from celery import Celery
from kombu import Queue, Exchange
from celery.signals import worker_ready, worker_shutdown
from config import Config
from api.app import create_app

logger = logging.getLogger(__name__)

def make_celery(flask_app):
    celery_app = Celery(
        __name__,
        broker=Config.CELERY_BROKER_URL,
        backend=Config.CELERY_RESULT_BACKEND,
        enable_utc=True,
    )

    # ---- HA & scaling horizontal
    celery_app.conf.update(
        # Fiabilité
        task_acks_late=True,                 # si le worker meurt, la tâche est re-queue
        worker_prefetch_multiplier=1,        # évite qu’un worker “aspire” trop de tâches
        task_time_limit=300,                 # tue les tasks coincées
        task_soft_time_limit=270,

        # Broker résilient
        broker_connection_retry_on_startup=True,
        broker_connection_retry=True,
        broker_connection_max_retries=100,

        # Sérialisation
        task_serializer='json',
        result_serializer='json',
        accept_content=['json'],
        result_expires=timedelta(hours=6),

        # Events (monitoring Flower/Prometheus exporter)
        worker_send_task_events=True,
        task_send_sent_event=True,
    )

    # ---- Files d’attente
    default_ex = Exchange('default', type='direct')
    beat_ex = Exchange('beat', type='direct')
    high_ex = Exchange('high_priority', type='direct')

    celery_app.conf.task_queues = (
        Queue('high_priority', high_ex, routing_key='high_priority',
              queue_arguments={'x-max-priority': 10}),
        Queue('beat_tasks', beat_ex, routing_key='beat.process'),
        Queue('default', default_ex, routing_key='default'),
    )

    celery_app.conf.task_default_queue = 'default'
    celery_app.conf.task_default_exchange = 'default'
    celery_app.conf.task_default_routing_key = 'default'

    celery_app.conf.task_routes = {
        'workflow.tasks.core.scheduler.process_scheduled_steps': {'queue': 'beat_tasks', 'routing_key': 'beat.process'},
        'workflow.tasks.operations.escalate.send_escalate_task': {'queue': 'high_priority', 'routing_key': 'high_priority', 'priority': 9},
        'workflow.tasks.operations.reminder.send_reminder_task': {'queue': 'default', 'routing_key': 'default', 'priority': 5},
    }

    # ---- Beat intégré (si tu gardes Beat “unique”)
    celery_app.conf.beat_schedule = {
        'process-scheduled-steps-every-5min': {
            'task': 'workflow.tasks.core.scheduler.process_scheduled_steps',
            'schedule': timedelta(seconds=300),
            'options': {'queue': 'beat_tasks'}
        }
    }

    # ---- Contexte Flask pour toutes les tasks
    class FlaskContextTask(celery_app.Task):
        def __call__(self, *args, **kwargs):
            with flask_app.app_context():
                return super().__call__(*args, **kwargs)

        def on_failure(self, exc, task_id, args, kwargs, einfo):
            logger.error(f"Task {task_id} failed: {exc}")
            return super().on_failure(exc, task_id, args, kwargs, einfo)

    celery_app.Task = FlaskContextTask

    # ---- Signaux (logs)
    @worker_ready.connect
    def worker_ready_handler(sender, **kw):
        logger.info(f"Worker {sender.hostname} ready")

    @worker_shutdown.connect
    def worker_shutdown_handler(sender, **kw):
        logger.info(f"Worker {sender.hostname} shutting down")

    return celery_app


flask_app = create_app(Config)
celery_app = make_celery(flask_app)

@celery_app.task(bind=True)
def health_check(self):
    from datetime import datetime
    return {
        "status": "OK",
        "worker": self.request.hostname,
        "timestamp": datetime.utcnow().isoformat()
    }
```

> **Variante RedBeat (Beat distribué)** : ajoute simplement
>
> ```python
> celery_app.conf.beat_scheduler = 'redbeat.RedBeatScheduler'
> celery_app.conf.redbeat_redis_url = Config.CELERY_BROKER_URL
> ```
>
> et tu peux lancer plusieurs Beat en parallèle.

---

# 2) `scheduler.py` — idempotence + lock pessimiste (OK)

Ton approche `with_for_update(skip_locked=True)` est la bonne. Petite retouche “defensive” :

```python
# workflow/tasks/core/scheduler.py
from sqlalchemy.exc import SQLAlchemyError
from workflow.celery_app import celery_app as app
from api.models import FormContainer, WorkflowStep
from api.extensions import db
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@app.task(bind=True)
def process_scheduled_steps(self) -> None:
    try:
        with db.session.begin():
            steps = (
                WorkflowStep.query
                .filter(
                    WorkflowStep.status == 'pending',
                    WorkflowStep.scheduled_time <= datetime.utcnow()
                )
                .with_for_update(skip_locked=True)
                .limit(100)
                .all()
            )

            for step in steps:
                container = FormContainer.query.get(step.container_id)
                if not container:
                    step.status = 'revoked'
                    continue

                # on marque processing TOUT DE SUITE, transaction ouverte
                step.status = 'processing'
                step.locked_at = datetime.utcnow()

                if step.step_type == 'reminder':
                    from workflow.tasks.operations.reminder import send_reminder_task
                    send_reminder_task.apply_async(args=[step.id, step.form_id, step.container_id, step.step_index])
                else:
                    from workflow.tasks.operations.escalate import send_escalate_task
                    send_escalate_task.apply_async(args=[step.id, step.form_id, step.container_id])

    except SQLAlchemyError as e:
        logger.error(f"Database error in step processing: {e}")
        db.session.rollback()
        self.retry(countdown=300)
    except Exception as e:
        logger.error(f"Unexpected error in step processing: {e}")
        self.retry(countdown=600)
```

---

# 3) `BaseWorkflowTask` — corrections + verrous + retries propres

* Corrige l’usage de `step.query` (il faut la classe `WorkflowStep`).
* Marque `processing` sous la même transaction avant de lancer le handler.
* Remonte correctement le statut/retries.

```python
# workflow/tasks/core/base.py
import logging
from functools import wraps
from typing import Dict, Callable
from datetime import datetime, timedelta
from celery import Task

from api.models import Form, FormContainer, WorkflowStep
from api.extensions import db
from sqlalchemy import select
from sqlalchemy.exc import SQLAlchemyError
from workflow.exceptions import RevokeChainRequested, WorkflowConfigurationError, WorkflowException

logger = logging.getLogger(__name__)

def workflow_task(max_retries: int = 3, retry_delay: int = 60):
    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            try:
                return func(self, *args, **kwargs)
            except RevokeChainRequested as e:
                logger.info(f"Chain revoked: {e}", extra=getattr(e, 'context', {}))
                return {'status': 'revoked', 'message': str(e)}
            except SQLAlchemyError as e:
                logger.error(f"Database error in {func.__name__}: {e}")
                self.retry(exc=e, countdown=retry_delay, max_retries=max_retries)
            except Exception as e:
                logger.error(f"Unexpected error in {func.__name__}: {e}")
                self.retry(exc=e, countdown=retry_delay, max_retries=max_retries)
        return wrapper
    return decorator


class BaseWorkflowTask(Task):
    autoretry_for = (Exception,)
    max_retries = 3
    retry_backoff = True
    retry_backoff_max = 600
    rate_limit = '10/m'

    def _handle_task(self, step_id: str, context: Dict, handler: Callable) -> Dict:
        # Lock pessimiste + marquage "processing" atomique
        with db.session.begin():
            step = db.session.execute(
                select(WorkflowStep)
                .where(WorkflowStep.id == step_id)
                .with_for_update(skip_locked=True)
            ).scalar_one_or_none()

            if not step:
                raise RevokeChainRequested("Step not found or locked")

            context.update({'step_id': step_id, 'task_id': self.request.id})

            self._validate_prerequisites(step, context)

            if step.status not in ('processing', 'pending'):
                raise WorkflowException(
                    f"Invalid step state: {step.status}",
                    {'current_status': step.status, **context}
                )

            # si le scheduler a déjà mis processing, on continue,
            # sinon on le met maintenant (cas d’appel direct)
            if step.status == 'pending':
                step.status = 'processing'
                step.locked_at = datetime.utcnow()

        try:
            result = handler(context)
            self._finalize_step(step_id, 'completed')
            return result
        except RevokeChainRequested as e:
            self._handle_revocation(step_id, context, e)
            return getattr(e, "return_value", {"status": "revoked", "message": str(e)})
        except Exception as e:
            self._handle_retry(step_id, e)
            raise

    def _validate_prerequisites(self, step: WorkflowStep, context: Dict) -> None:
        form = Form.query.get(context['form_id'])
        container = FormContainer.query.get(context['container_id'])
        if not form or not container:
            raise RevokeChainRequested("Form or container missing", {'status': 'revoked', **context})

    def _finalize_step(self, step_id: str, status: str) -> None:
        with db.session.begin():
            step = db.session.get(WorkflowStep, step_id)
            if step:
                step.status = status

    def _handle_revocation(self, step_id: str, context: Dict, error: RevokeChainRequested) -> None:
        with db.session.begin():
            step = db.session.get(WorkflowStep, step_id)
            if step:
                logger.warning(f"Revoking workflow chain: {error}", extra=context)
                # on révoque uniquement ce step :
                step.status = 'revoked'

    def _handle_retry(self, step_id: str, error: Exception) -> None:
        with db.session.begin():
            step = db.session.get(WorkflowStep, step_id)
            if step:
                step.retries += 1
                if step.retries >= step.max_retries:
                    step.status = 'failed'
                else:
                    step.status = 'pending'
                    step.scheduled_time = datetime.utcnow() + timedelta(hours=1)
```

---

# 4) **Fix des tasks “operations”** — bugs concrets

Tu as deux bugs qui provoqueront des erreurs + des emails non conformes :

* Typos/propriétés : `appication` → `application`.
* Appel de `MailManager.send_email(...)` **mauvais ordre/arguments**.
  Ta signature est :

  ```python
  def send_email(form_container, template_type, recipient_email=None, cc_emails=None,
                 substitutions=None, workflow_step='start'):
  ```

  → Il faut lui passer **`form_container=`** et **`template_type=`**, pas `mail_sender`/`subject`/`token` en positions.

### `escalate.py` (corrigé)

```python
# workflow/tasks/operations/escalate.py
import logging
from typing import Dict
from datetime import datetime
from api.models import FormContainer
from api.helpers.tools import log_timeline_event
from workflow.emails.email_manager import MailManager
from workflow.celery_app import celery_app as app
from workflow.tasks.core.base import BaseWorkflowTask, workflow_task

logger = logging.getLogger(__name__)

@app.task(base=BaseWorkflowTask, bind=True)
@workflow_task()
def send_escalate_task(self, step_id: str, form_id: str, container_id: str) -> Dict:
    def task_handler(context: Dict) -> Dict:
        form_container: FormContainer = FormContainer.query.get(context['container_id'])

        # envoi via template "escalate"
        MailManager.send_email(
            form_container=form_container,
            template_type='escalate',
            recipient_email=form_container.escalade_email,
            cc_emails=form_container.cc_emails,
            substitutions={'workflow_reason': 'Escalation Required'},
            workflow_step='escalate'
        )

        log_timeline_event(
            form_container,
            form_id,
            datetime.utcnow(),
            "Escalation triggered",
            'escalate'
        )
        return {'status': 'success', 'escalation': True}

    return self._handle_task(step_id, {
        'form_id': form_id,
        'container_id': container_id
    }, task_handler)
```

### `reminder.py` (corrigé)

```python
# workflow/tasks/operations/reminder.py
import logging
from typing import Dict
from datetime import datetime
from api.models import FormContainer
from api.helpers.tools import log_timeline_event
from workflow.emails.email_manager import MailManager
from workflow.celery_app import celery_app as app
from workflow.tasks.core.base import BaseWorkflowTask, workflow_task

logger = logging.getLogger(__name__)

@app.task(base=BaseWorkflowTask, bind=True)
@workflow_task()
def send_reminder_task(self, step_id: str, form_id: str, container_id: str, reminder_count: int) -> Dict:
    def task_handler(context: Dict) -> Dict:
        form_container: FormContainer = FormContainer.query.get(context['container_id'])

        MailManager.send_email(
            form_container=form_container,
            template_type='reminder',
            recipient_email=form_container.user_email,
            cc_emails=form_container.cc_emails,
            substitutions={'reminder_count': reminder_count},
            workflow_step='reminder'
        )

        log_timeline_event(
            form_container,
            form_id,
            datetime.utcnow(),
            f"Reminder {reminder_count} sent",
            'reminder'
        )
        return {'status': 'success', 'reminder_count': reminder_count}

    return self._handle_task(step_id, {
        'form_id': form_id,
        'container_id': container_id,
        'reminder_count': reminder_count
    }, task_handler)
```

---

# 5) `email_manager.py` — OK mais deux micro-améliorations

* Sécu : transforme `to`/`cc` en listes filtrées (non-vides).
* Fail-fast si aucun destinataire.

```python
# workflow/emails/email_manager.py (ajouts mineurs)
# ... (le reste inchangé)

        to = (recipient_email or form_container.user_email) or ""
        cc = cc_emails or form_container.cc_emails or []

        recipients = [to] if isinstance(to, str) and to.strip() else list(filter(None, to or []))
        cc_list = list({e for e in (cc or []) if e})

        if not recipients:
            raise ValueError("No recipient email resolved")

        msg = Message(
            subject=subject,
            sender=mail_sender,
            recipients=recipients,
            cc=cc_list
        )
        msg.html = full_html
        mail.send(msg)
```

---

# 6) PostgreSQL — réglages runtime recommandés

Dans `SQLALCHEMY_ENGINE_OPTIONS` (tu l’as déjà bien amorcé) :

```python
SQLALCHEMY_ENGINE_OPTIONS = {
    'pool_size': 20,
    'max_overflow': 30,
    'pool_pre_ping': True,
    'pool_recycle': 3600,
    # Optionnel : isolation plus stricte si besoin
    # 'isolation_level': 'READ COMMITTED',  # défaut PG
}
```

Et côté PostgreSQL (paramètres serveur) :

* `deadlock_timeout = '1s'`
* `lock_timeout = '5s'` (si tu veux fail-fast sur locks)
* Index utiles sur `(status, scheduled_time)` de `WorkflowStep`.

---

# 7) Déploiement

* **Workers (N≥2)**, **un seul Beat** (ou **RedBeat** si tu veux N Beats).
* Backends Flask derrière LB.
* DB **master/replica** (écritures sur master).
* Redis/RabbitMQ HA (sentinels/cluster).

---

Si tu veux, je te prépare un **diff prêt à coller** (patch unified) pour tes fichiers actuels, ou un **docker-compose** minimal “prod-like” (web + worker x2 + beat + postgres + redis + nginx).






Parfait 👍 tu veux une archi **multi-serveurs (actif-actif)** avec :

* **2 serveurs** (ou +) où tournent tes workers **et** Beat,
* **un Redis global en cluster** (ou en Sentinel) qui sert à la fois de broker, backend, et scheduler distribué via **RedBeat**.

Dans ce cas, tout ton code peut rester le même à une exception près : **`celery_app.py` doit être configuré pour RedBeat + Redis cluster**.

---

## 🔹 `workflow/celery_app.py` – version actif-actif multi-serveurs

```python
from datetime import timedelta, datetime
import logging
from celery import Celery
from kombu import Queue, Exchange
from celery.signals import worker_ready, worker_shutdown
from config import Config
from api.app import create_app

logger = logging.getLogger(__name__)

def make_celery(flask_app):
    celery_app = Celery(
        __name__,
        broker=Config.CELERY_BROKER_URL,        # Redis cluster ou Sentinel
        backend=Config.CELERY_RESULT_BACKEND,  # Redis aussi
        enable_utc=True,
    )

    # -------------------------
    #  Configuration global HA
    # -------------------------
    celery_app.conf.update(
        task_acks_late=True,
        worker_prefetch_multiplier=1,
        broker_connection_retry_on_startup=True,
        broker_connection_retry=True,
        broker_connection_max_retries=100,
        task_serializer='json',
        result_serializer='json',
        accept_content=['json'],
        result_expires=timedelta(hours=6),
        worker_send_task_events=True,
        task_send_sent_event=True,
    )

    # -------------------------
    #  RedBeat (Beat distribué)
    # -------------------------
    celery_app.conf.beat_scheduler = 'redbeat.RedBeatScheduler'
    celery_app.conf.redbeat_redis_url = Config.CELERY_BROKER_URL
    celery_app.conf.redbeat_lock_key = 'redbeat:lock'
    celery_app.conf.redbeat_key_prefix = 'redbeat:'

    celery_app.conf.beat_schedule = {
        'process-scheduled-steps': {
            'task': 'workflow.tasks.core.scheduler.process_scheduled_steps',
            'schedule': timedelta(seconds=300),
            'options': {'queue': 'beat_tasks'}
        }
    }

    # -------------------------
    #  Queues dédiées
    # -------------------------
    default_ex = Exchange('default', type='direct')
    beat_ex = Exchange('beat', type='direct')
    high_ex = Exchange('high_priority', type='direct')

    celery_app.conf.task_queues = (
        Queue('high_priority', high_ex, routing_key='high_priority',
              queue_arguments={'x-max-priority': 10}),
        Queue('beat_tasks', beat_ex, routing_key='beat.process'),
        Queue('default', default_ex, routing_key='default'),
    )

    celery_app.conf.task_default_queue = 'default'
    celery_app.conf.task_routes = {
        'workflow.tasks.core.scheduler.process_scheduled_steps': {'queue': 'beat_tasks'},
        'workflow.tasks.operations.escalate.send_escalate_task': {'queue': 'high_priority'},
        'workflow.tasks.operations.reminder.send_reminder_task': {'queue': 'default'},
    }

    # -------------------------
    #  Contexte Flask
    # -------------------------
    class FlaskContextTask(celery_app.Task):
        def __call__(self, *args, **kwargs):
            with flask_app.app_context():
                return super().__call__(*args, **kwargs)

        def on_failure(self, exc, task_id, args, kwargs, einfo):
            logger.error(f"Task {task_id} failed: {exc}")
            return super().on_failure(exc, task_id, args, kwargs, einfo)

    celery_app.Task = FlaskContextTask

    # -------------------------
    #  Signaux log
    # -------------------------
    @worker_ready.connect
    def worker_ready_handler(sender, **kw):
        logger.info(f"Worker {sender.hostname} ready")

    @worker_shutdown.connect
    def worker_shutdown_handler(sender, **kw):
        logger.info(f"Worker {sender.hostname} shutting down")

    return celery_app


flask_app = create_app(Config)
celery_app = make_celery(flask_app)

@celery_app.task(bind=True)
def health_check(self):
    return {
        "status": "OK",
        "worker": self.request.hostname,
        "timestamp": datetime.utcnow().isoformat()
    }
```

---

## 🔹  Déploiement sur 2 serveurs (actif-actif)

👉 Tu lances **les mêmes process sur les 2 machines** :

```bash
# Sur les 2 serveurs
celery -A workflow.celery_app.celery_app worker -Q default,high_priority,beat_tasks -l INFO -n worker@%h
celery -A workflow.celery_app.celery_app beat -l INFO -s /tmp/celerybeat-schedule
```

* Grâce à **RedBeat**, les 2 Beat vont se coordonner → un seul exécute le job toutes les 5 min, l’autre est standby.
* Grâce à Redis cluster, si un nœud tombe, tout continue.
* Les workers consomment en parallèle sans doublons (grâce à `with_for_update(skip_locked)` dans tes tasks DB).

---

## 🔹 Avantages

✅ **Pas de SPOF Beat** → tu peux lancer un Beat sur chaque serveur.
✅ **Redis cluster global** → broker + backend + Beat distribué, robuste.
✅ **Workers scalables** → chaque serveur peut en héberger plusieurs.
✅ **DB sûre** → `skip_locked` évite les doublons.

---

Veux-tu que je te prépare un **docker-compose multi-nodes** (2 serveurs simulés, 1 Redis cluster, Celery workers + Beat) pour que tu testes l’archi avant de passer en infra réelle ?
