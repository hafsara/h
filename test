# workflow/celery_app.py
from datetime import timedelta
import logging
from celery import Celery
from kombu import Queue, Exchange
from celery.signals import worker_ready, worker_shutdown
from config import Config
from api.app import create_app

logger = logging.getLogger(__name__)

def make_celery(flask_app):
    celery_app = Celery(
        __name__,
        broker=Config.CELERY_BROKER_URL,
        backend=Config.CELERY_RESULT_BACKEND,
        enable_utc=True,
    )

    # ---- Configuration pour Redis Cluster
    celery_app.conf.update(
        # Fiabilit√©
        task_acks_late=True,
        worker_prefetch_multiplier=1,
        task_time_limit=300,
        task_soft_time_limit=270,

        # Broker r√©silient pour cluster
        broker_connection_retry_on_startup=True,
        broker_connection_retry=True,
        broker_connection_max_retries=100,
        broker_transport_options={
            'visibility_timeout': 3600,  # Important pour cluster
            'fanout_prefix': True,
            'fanout_patterns': True,
            'global_keyprefix': 'celery:',
            'socket_keepalive': True,
            'retry_on_timeout': True,
            'max_connections': 10
        },

        # S√©rialisation
        task_serializer='json',
        result_serializer='json',
        accept_content=['json'],
        result_expires=timedelta(hours=6),

        # Events
        worker_send_task_events=True,
        task_send_sent_event=True,
        
        # üî• CONFIGURATION BEAT POUR REDIS CLUSTER
        beat_max_loop_interval=30,  # V√©rification plus fr√©quente
        beat_sync_every=1,  # Synchronisation imm√©diate
    )

    # ---- RedBeat Scheduler (RECOMMAND√â pour cluster)
    if getattr(Config, 'REDBEAT_ENABLED', True):
        celery_app.conf.update(
            beat_scheduler='redbeat.RedBeatScheduler',
            redbeat_redis_url=Config.CELERY_BROKER_URL,
            redbeat_key_prefix='redbeat:',
            redbeat_lock_key='redbeat:lock',
            redbeat_lock_timeout=60,  # Verrou de 60s
        )
        logger.info("RedBeat scheduler configured for Redis Cluster")
    else:
        # Scheduler par d√©faut (moins recommand√© pour cluster)
        celery_app.conf.beat_schedule = {
            'process-scheduled-steps-every-5min': {
                'task': 'workflow.tasks.core.scheduler.process_scheduled_steps',
                'schedule': timedelta(seconds=300),
                'options': {'queue': 'beat_tasks'}
            },
            'distributed-health-check': {
                'task': 'workflow.celery_app.distributed_health_check',
                'schedule': timedelta(seconds=60),
                'options': {'queue': 'beat_tasks'}
            }
        }

    # ---- Files d'attente
    default_ex = Exchange('default', type='direct')
    beat_ex = Exchange('beat', type='direct')
    high_ex = Exchange('high_priority', type='direct')

    celery_app.conf.task_queues = (
        Queue('high_priority', high_ex, routing_key='high_priority',
              queue_arguments={'x-max-priority': 10}),
        Queue('beat_tasks', beat_ex, routing_key='beat.process'),
        Queue('default', default_ex, routing_key='default'),
    )

    celery_app.conf.task_default_queue = 'default'
    celery_app.conf.task_default_exchange = 'default'
    celery_app.conf.task_default_routing_key = 'default'

    celery_app.conf.task_routes = {
        'workflow.tasks.core.scheduler.process_scheduled_steps': {
            'queue': 'beat_tasks', 
            'routing_key': 'beat.process'
        },
        'workflow.tasks.operations.escalate.send_escalate_task': {
            'queue': 'high_priority', 
            'routing_key': 'high_priority', 
            'priority': 9
        },
        'workflow.tasks.operations.reminder.send_reminder_task': {
            'queue': 'default', 
            'routing_key': 'default', 
            'priority': 5
        },
    }

    # ---- Contexte Flask pour toutes les tasks
    class FlaskContextTask(celery_app.Task):
        def __call__(self, *args, **kwargs):
            with flask_app.app_context():
                return super().__call__(*args, **kwargs)

        def on_failure(self, exc, task_id, args, kwargs, einfo):
            logger.error(f"Task {task_id} failed: {exc}")
            return super().on_failure(exc, task_id, args, kwargs, einfo)

    celery_app.Task = FlaskContextTask

    # ---- Signaux avec coordination
    @worker_ready.connect
    def worker_ready_handler(sender, **kw):
        logger.info(f"Worker {sender.hostname} ready")
        # Enregistrement du worker
        try:
            with flask_app.app_context():
                flask_app.redis_client.register_server_heartbeat(sender.hostname)
        except Exception as e:
            logger.error(f"Failed to register worker heartbeat: {e}")

    @worker_shutdown.connect
    def worker_shutdown_handler(sender, **kw):
        logger.info(f"Worker {sender.hostname} shutting down")
        # Nettoyage optionnel
        try:
            with flask_app.app_context():
                # Le TTL g√®re automatiquement l'expiration
                pass
        except Exception as e:
            logger.error(f"Error during worker shutdown: {e}")

    return celery_app

# Initialisation
flask_app = create_app(Config)
celery_app = make_celery(flask_app)
